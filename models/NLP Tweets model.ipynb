{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(32, activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer with 1 neuron and Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Assuming you've already trained an XGBoost model named 'model' and X_train is your training data\n",
    "# 'features' should be a list of the indices or names of the features you want to plot\n",
    "PartialDependenceDisplay(booster, X_train, feature_importances, grid_resolution=100)\n",
    "plt.show()\n",
    "# Assuming you've already trained an XGBoost model named 'model' and X_test is your testing data\n",
    "perm_importance = permutation_importance(model, X_test, y_test)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot permutation feature importance\n",
    "plt.barh(feature_importances[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_words(text):\n",
    "    # Initialize a defaultdict to store word counts\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Convert text to lowercase and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count occurrences of each word\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "    \n",
    "    return dict(word_counts)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample sentence..re.. This sentence contains sample words.\"\n",
    "word_count_dict = count_words(text)\n",
    "print(word_count_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+|http://t\\.co/\\S+'\n",
    "    # Replace URLs found in the text with an empty string (i.e., remove them)\n",
    "    no_url_text = re.sub(url_pattern, '', text)\n",
    "    return no_url_text\n",
    "\n",
    "\n",
    "# Tokenization and feature extraction\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "X_tweets = dataset_tweets.drop(columns=['IsBot'])\n",
    "# Convert NaN values to empty strings\n",
    "X_tweets['text'] = X_tweets['text'].fillna('')\n",
    "\n",
    "X_tweets['clean_text'] = X_tweets['text'].apply(remove_urls)\n",
    "X_tweets['clean_text'] = X_tweets['clean_text'].astype(str)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "X_tweets.drop(columns=['text'], inplace=True)\n",
    "\n",
    "\n",
    "X_vectorized = vectorizer.fit_transform(X_tweets['clean_text'])\n",
    "print(X_tweets)\n",
    "print(X_vectorized)\n",
    "\n",
    "#print(vectorizer.get_feature_names_out())\n",
    "#vectorizer.fit(X_tweets)\n",
    "#print(\"Vocabulary: \", vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample DataFrame with text data\n",
    "data = {\n",
    "    'text': [\n",
    "        'This is the first document.',\n",
    "        'This document is the second document.',\n",
    "        'And this is the third one.',\n",
    "        'Is this the first document?',\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data to a token count matrix\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert the matrix to an array and print the result\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names (vocabulary)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
